{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoiceDub - YouTube Dubbing Backend (GPU)\n",
    "\n",
    "This notebook runs the dubbing backend on Google Colab's free T4 GPU.\n",
    "\n",
    "**Setup:**\n",
    "1. Go to **Runtime > Change runtime type > T4 GPU**\n",
    "2. Run all cells below\n",
    "3. Copy the `ngrok` public URL and paste it in your frontend\n",
    "\n",
    "**Features:** Chatterbox TTS (human-like voice) + Whisper (GPU transcription) + Gemini translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1. Check GPU\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 2. Clone repo & install dependencies\nimport os\n\nif os.path.exists('/content/app'):\n    !cd /content/app && git pull\nelse:\n    !git clone https://github.com/sasmalgiri/youtube-dubbing.git /content/app\n\n# Install build tools\n!apt-get -qq install -y libsndfile1 > /dev/null 2>&1\n\n# Install chatterbox deps manually (pip build fails on Colab)\n!pip install -q numpy==1.26.4\n!pip install -q \"librosa>=0.11.0\" \"s3tokenizer\" \"torch>=2.6.0\" \"torchaudio>=2.6.0\" \\\n    \"transformers==4.46.3\" \"diffusers==0.29.0\" \"resemble-perth>=1.0.1\" \\\n    \"conformer>=0.3.2\" \"safetensors>=0.5.3\" \"spacy-pkuseg\" \"pykakasi>=2.3.0\" \\\n    \"pyloudnorm\" \"omegaconf\"\n!pip install -q chatterbox-tts --no-deps\n\n# Install remaining backend deps\n!pip install -q fastapi uvicorn[standard] python-multipart pydantic edge-tts \\\n    faster-whisper deep-translator google-genai openai elevenlabs \\\n    sse-starlette rich yt-dlp pyngrok\n\n# Restart runtime so numpy 1.26.4 loads properly\nprint(\"\\nAll installed! Restarting runtime for numpy fix...\")\nprint(\"After restart, skip this cell and run cells 3, 4, 5, 6.\")\nimport os\nos._exit(0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 3. Set API Keys + Upload YouTube Cookies\n#@markdown Enter your API keys below:\n\nGEMINI_API_KEY = \"\" #@param {type:\"string\"}\nELEVENLABS_API_KEY = \"\" #@param {type:\"string\"}\nOPENAI_API_KEY = \"\" #@param {type:\"string\"}\nNGROK_AUTH_TOKEN = \"\" #@param {type:\"string\"}\nNGROK_DOMAIN = \"\" #@param {type:\"string\"}\n\n#@markdown ---\n#@markdown **Required:** `GEMINI_API_KEY` + `NGROK_AUTH_TOKEN`\n#@markdown\n#@markdown **Optional:** `NGROK_DOMAIN`, `ELEVENLABS_API_KEY`, `OPENAI_API_KEY`\n\nimport os\nos.environ['GEMINI_API_KEY'] = GEMINI_API_KEY\nif ELEVENLABS_API_KEY:\n    os.environ['ELEVENLABS_API_KEY'] = ELEVENLABS_API_KEY\nif OPENAI_API_KEY:\n    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n\nwith open('/content/app/backend/.env', 'w') as f:\n    f.write(f'GEMINI_API_KEY={GEMINI_API_KEY}\\n')\n    if ELEVENLABS_API_KEY:\n        f.write(f'ELEVENLABS_API_KEY={ELEVENLABS_API_KEY}\\n')\n    if OPENAI_API_KEY:\n        f.write(f'OPENAI_API_KEY={OPENAI_API_KEY}\\n')\n\nprint(\"API keys set!\")\n\n# Upload YouTube cookies (required — YouTube blocks Colab IPs)\nprint(\"\\n--- YouTube Cookies ---\")\nprint(\"YouTube blocks downloads from Colab. You need to upload cookies.\")\nprint(\"Steps:\")\nprint(\"  1. Install 'Get cookies.txt LOCALLY' Chrome extension\")\nprint(\"  2. Go to youtube.com (make sure you're logged in)\")\nprint(\"  3. Click the extension → Export → saves cookies.txt\")\nprint(\"  4. Upload it below:\\n\")\n\nfrom google.colab import files\nuploaded = files.upload()\nfor name, data in uploaded.items():\n    with open('/content/app/backend/cookies.txt', 'wb') as f:\n        f.write(data)\n    print(f\"\\nCookies saved! ({name}, {len(data)} bytes)\")\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 4. Pre-download Whisper model + fix yt-dlp\n%cd /content/app/backend\n\n# Install deno (required by yt-dlp for YouTube extraction)\n!curl -fsSL https://deno.land/install.sh | sh\nimport os\nos.environ[\"PATH\"] = \"/root/.deno/bin:\" + os.environ[\"PATH\"]\n\n# Verify yt-dlp works\n!yt-dlp --version\nprint(\"yt-dlp + deno ready!\")\n\n# Pre-download Whisper model\nfrom faster_whisper import WhisperModel\nprint(\"\\nDownloading Whisper 'small' model...\")\nmodel = WhisperModel(\"small\", device=\"cuda\", compute_type=\"float16\")\ndel model\nprint(\"Whisper model cached!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "#@title 5. Start Backend Server + ngrok Tunnel\nimport subprocess\nimport time\nimport os\nfrom pyngrok import ngrok, conf\n\n# Ensure deno is in PATH for yt-dlp\nos.environ[\"PATH\"] = \"/root/.deno/bin:\" + os.environ.get(\"PATH\", \"\")\n\n# Set ngrok auth token\nif NGROK_AUTH_TOKEN:\n    conf.get_default().auth_token = NGROK_AUTH_TOKEN\nelse:\n    raise ValueError(\"Missing NGROK_AUTH_TOKEN\")\n\n# Start uvicorn in background (inherits PATH with deno)\nproc = subprocess.Popen(\n    ['python', '-m', 'uvicorn', 'app:app', '--host', '0.0.0.0', '--port', '8000'],\n    cwd='/content/app/backend',\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    env=os.environ.copy(),\n)\ntime.sleep(3)\n\n# Use static domain if provided\nif NGROK_DOMAIN:\n    public_url = ngrok.connect(8000, \"http\", domain=NGROK_DOMAIN)\n    url = f\"https://{NGROK_DOMAIN}\"\nelse:\n    public_url = ngrok.connect(8000, \"http\")\n    url = str(public_url)\n\nprint(\"=\" * 60)\nprint(\"Backend running on GPU!\")\nprint(f\"\\nPUBLIC URL: {url}\")\nprint(f\"\\nTest: {url}/api/health\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Monitor Server Logs (run this to see live output)\n",
    "#@markdown Keep this cell running to see backend logs in real-time.\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"Monitoring server... (this cell keeps running)\")\n",
    "print(\"Submit a dubbing job from your frontend to see progress here.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "try:\n",
    "    while proc.poll() is None:\n",
    "        line = proc.stdout.readline()\n",
    "        if line:\n",
    "            print(line.decode('utf-8', errors='replace').rstrip())\n",
    "        else:\n",
    "            time.sleep(0.5)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nStopped monitoring (server still running)\")"
   ]
  }
 ]
}